{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "# 02 Regression with PyTorch \n",
    "## Introduction\n",
    "In this exercise, we fit a neural network model to 1D data.\n",
    "\n",
    "We shall use the machine learning model \n",
    "\n",
    "$$f(\\mathbf{x}, \\theta) = \\,\\mathbf{b}_1 + \\mathbf{w}_1 \\, \\mbox{relu}(\\mathbf{b}_0 + \\mathbf{w}_0 \\, \\mathbf{x}),$$\n",
    "\n",
    "where $\\mathbf{b}$ and $\\mathbf{w}$ (the biases and weights) are the parameters $\\theta$ of the model and $\\mbox{relu}(x)$ is a function applied to every element $x_i$ of its tensor argument (i.e., applied element-wise) defined by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{relu}(x) & = \\begin{cases}\n",
    "    x, & \\text{if } x \\gt 0\\\\\n",
    "    0              & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FA1Y5VCv20XZ"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  a function to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "#  split data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>0.819436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>-0.558397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>-0.660444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>-0.033374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>0.776216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>-0.875471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>-0.881089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>0.376558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>0.354957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.779505</td>\n",
       "      <td>0.159820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target         x\n",
       "0  2.779505  0.819436\n",
       "1  2.779505 -0.558397\n",
       "2  2.779505 -0.660444\n",
       "3  2.779505 -0.033374\n",
       "4  2.779505  0.776216\n",
       "5  2.779505 -0.875471\n",
       "6  2.779505 -0.881089\n",
       "7  2.779505  0.376558\n",
       "8  2.779505  0.354957\n",
       "9  2.779505  0.159820"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = jb.load('data_02.db')\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test sets\n",
    "There is some confusion in terminology regarding validation and test samples (or sets). We shall adhere to the defintions given here https://machinelearningmastery.com/difference-test-validation-datasets/):\n",
    "   \n",
    "  * __Training Dataset__: The sample of data used to fit the model.\n",
    "  * __Validation Dataset__: The sample of data used to decide 1) whether the fit is reasonable (e.g., the model has not been overfitted), 2) decide which of several models is the best and 3) tune model hyperparameters.\n",
    "  * __Test Dataset__: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "The validation set will be some small fraction of the training set and will be used to decide when to stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:         35000\n",
      "validation set size:     5000\n",
      "test set size:          10000\n"
     ]
    }
   ],
   "source": [
    "# Fraction of the data assigned as test data\n",
    "fraction = 0.20\n",
    "# Split data into a part for training and a part for testing\n",
    "train_data, test_data = train_test_split(data, test_size=fraction)\n",
    "\n",
    "# Split the training data into a part for training (fitting) and\n",
    "# a part for validating the training.\n",
    "fraction = 0.125\n",
    "train_data, valid_data = train_test_split(train_data, test_size=fraction)\n",
    "\n",
    "print('train set size:        %6d' % train_data.shape[0])\n",
    "print('validation set size:   %6d' % valid_data.shape[0])\n",
    "print('test set size:         %6d' % test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into targets $t$ and inputs $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95300978 -0.60370799  0.06827101]\n",
      "\n",
      "[-0.59446741  0.49942012  0.78096059]\n",
      "\n",
      "[0.73273797 0.8346517  0.99478322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_t_x(df):\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    t = np.array(df['target'])\n",
    "    x = np.array(df['x'])\n",
    "    return (t, x)\n",
    "\n",
    "train_t, train_x = split_t_x(train_data)\n",
    "valid_t, valid_x = split_t_x(valid_data)\n",
    "test_t,  test_x  = split_t_x(test_data)\n",
    "\n",
    "print(train_x[:3], end='\\n\\n') \n",
    "print(valid_x[:3], end='\\n\\n')\n",
    "print(test_x[:3],  end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([1, 0])\n",
      "tensor([[2.],\n",
      "        [3.]])\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# create a float tensor\n",
    "a = torch.Tensor([[1,2], [3,4]])\n",
    "print(a)\n",
    "\n",
    "# create an integer tensor, which we'll use for gathering  \n",
    "# elements of the [2,2] tensor 'a' along its dim=1 \n",
    "# (that is, horizontal) dimension.\n",
    "i = torch.tensor([1,0])\n",
    "print(i)\n",
    "\n",
    "# note that since we're gathering along the dim=1 dimension\n",
    "# of 'a' the tensor 'i' must be of the same shape as 'a'\n",
    "# along other dimensions besides dim=1, that is, along dim=0.\n",
    "# by using view(-1,...) we make the dim=0 (vertical)\n",
    "# dimension of i be of the same length as the dim=0 \n",
    "# dimension of 'a'.\n",
    "b  = a.gather(dim=1, index=i.view(-1, 1))\n",
    "print(b)\n",
    "\n",
    "# now we get rid of extraneous dimensions.\n",
    "# compare tensors 'b' and 'c'. in 'b' there is only one \n",
    "# element per row along dim=1, so its shape is [2, 1]. \n",
    "# We are, therefore, free to squeeze away the dim=1 to\n",
    "# arrive at a new tensor 'c' with shape [2,]\n",
    "c = b.squeeze()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return a (random) batch of data from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"batch_size\" integers from \n",
    "    # the range [0, length-1] corresponding to the\n",
    "    # row indices.\n",
    "    rows    = rnd.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    return (batch_x, batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical risk (that is, average loss)\n",
    "\n",
    "The empirical risk, which is the __objective function__ we shall minimize, is defined as\n",
    "\n",
    "\\begin{align}\n",
    "R_M(\\theta) & = \\frac{1}{M}\\sum_{m=1}^M L(t_m, f_m),\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "    f_m & \\equiv f(\\mathbf{x}_m, \\theta),\\\\ \\\\ \\textrm{and} \\\\\n",
    "    L(t, f) &= (t - f)^2\n",
    "\\end{align}\n",
    "\n",
    "The empirical risk $R_M$ approximates the __risk__\n",
    "\n",
    "\\begin{align}\n",
    "R[f] & = \\int \\cdots \\int \\, p(t, \\mathbf{x}) \\, L(t, f(\\mathbf{x}, \\theta)) \\, dt \\, d\\mathbf{x},\n",
    "\\end{align}\n",
    "\n",
    "which is a __functional__ of the model $f$. The quantity $p(t, \\mathbf{x}) \\, dt\\, d\\mathbf{x}$ is the probability distribution from which the sample $\\{ (t_m, \\mathbf{x}_m), m = 1,\\cdots, M \\}$ is presumed to have been drawn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_loss(f, t):\n",
    "  \n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(f, t):\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to execute training loop\n",
    "\n",
    "Note, here we use $t$ and $y$ interchangeably to denote the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, avloss, getbatch,\n",
    "          train_x, train_t, \n",
    "          valid_x, valid_t,\n",
    "          batch_size, \n",
    "          n_iterations, step=10):\n",
    "    \n",
    "    xx   = []\n",
    "    yy_t = []\n",
    "    yy_v = []\n",
    "    n    = 5000\n",
    "    \n",
    "    # set mode to training so that training specific operations such \n",
    "    # as dropout are enabled.\n",
    "    model.train()\n",
    "    \n",
    "    print(\"%10s\\t%10s\\t%10s\" % \\\n",
    "          ('iteration', 'train-set', 'valid-set'))\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "\n",
    "        # Get a random sample (a batch) of images (as numpy arrays)\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        \n",
    "        # Convert the numpy arrays batch_x and batch_t, to tensor \n",
    "        # types. The PyTorch tensor type is the magic that permits \n",
    "        # automatic differentiation with respect to parameters. \n",
    "        # However, since we do not need to take the derivatives\n",
    "        # with respect to x and t, we disable this feature\n",
    "        with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).long()      \n",
    "\n",
    "        # compute the output of the model for the batch of data x\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk = avloss(outputs, t)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        empirical_risk.backward()   # compute gradients\n",
    "        \n",
    "        # Finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer.step()            # move one step\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            acc_t = validate(model, train_x[:n], train_t[:n]) \n",
    "            acc_v = validate(model, valid_x[:n], valid_t[:n])\n",
    "            \n",
    "            print(\"\\r%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v), end='')\n",
    "        \n",
    "            xx.append(ii)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "    return (xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, inputs, targets):\n",
    "    # make sure we set evaluation mode so that training specific\n",
    "    # operations such as dropout are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "        # compute accuracy using training sample\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).long()\n",
    "        o = model(x)\n",
    "    return accuracy(o, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib has two graphics systems: 1) function-based and 2) object-based. The function below (plot_accuracy) illustrates the function-based system, while plot_empirical_risk illustrates the object-based system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(traces):\n",
    "    \n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    # adjust y limits\n",
    "    axes = ax.axes\n",
    "    axes.set_ylim((0, 1))\n",
    "    axes.set_xlim((0, xx[-1]))\n",
    "    \n",
    "    plt.plot(xx, yy_t, 'b', label='Training')\n",
    "    plt.plot(xx, yy_v, 'r', label='Validation')\n",
    "    plt.title('Training and Validation Errors')\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.grid(True, which=\"both\", linestyle='-')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_empirical_risk(xx, yy_t, yy_v):\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "\n",
    "    ax.set_title(\"Average loss function\")\n",
    "    \n",
    "    ax.plot(xx, yy_t, 'b', lw=2, label='Training')\n",
    "    ax.plot(xx, yy_v, 'r', lw=2, label='Validation')\n",
    "\n",
    "    ax.set_xlabel('Iterations', fontsize=FONTSIZE)\n",
    "    ax.set_ylabel('loss', fontsize=FONTSIZE)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, which=\"both\", linestyle='-')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJC916BU-9L6"
   },
   "source": [
    "### Define model $f(\\mathbf{x}, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRVXyg9A1zQh",
    "outputId": "aa498166-4008-461a-c82a-9eab5b49b083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "N_INPUTS  = 2       # number of inputs (x1, x2)\n",
    "N_NODES_0 = 2       # number of nodes in layer 0     \n",
    "N_OUTPUTS = 1\n",
    "\n",
    "# Instead of create our own class, let's just use the Sequential class.\n",
    "# Try to guess what's going on here.\n",
    "model = torch.nn.Sequential(\n",
    "             torch.nn.Linear(N_INPUTS,  N_NODES_0),\n",
    "             torch.nn.ReLU(),\n",
    "             torch.nn.Linear(N_NODES_0, N_OUTPUTS),\n",
    "             torch.nn.Sigmoid()\n",
    "     )\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8R55NgQL4EC"
   },
   "source": [
    "Instantiate an optimizer, then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration\t train-set\t valid-set\n",
      "      3010\t    0.5023\t    0.4998"
     ]
    }
   ],
   "source": [
    "n_batch = 10\n",
    "n_iterations  = 100\n",
    "learning_rate = 3.e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "traces = train(model, optimizer, \n",
    "               average_loss,\n",
    "               get_batch,\n",
    "               train_x, train_t, \n",
    "               valid_x, valid_t,\n",
    "               n_batch, \n",
    "               n_iterations,\n",
    "               step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_19_REINFORCE_Algorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
