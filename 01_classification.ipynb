{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "# 01 Classification with PyTorch \n",
    "## Introduction\n",
    "In this exercise, we consider the task of classifying data comprising two classes using __supervised learning__. In supervised learning a model is trained (that is, fitted) to data consisting of (ideally) unambiguously labeled data. For classification, the label is the class to which a given data vector (also known as a __feature vector__) belongs, while for regression the labels are the values associated with the feature vectors. \n",
    "A feature vector could be, for example, the pixel values of a flattened 2D image.\n",
    "\n",
    "You will build a __binary__ classifier in which one class is labeled with the __target__ $t = 1$ while the other is labeled with the target $t = 0$ and each object to be classified is characterized by data $\\mathbf{x}$. That is, the data set is $\\{(t_i, \\mathbf{x}_i \\}$. The fundamental assumption of almost all machine learning models is that the data are presumed to be drawn from a joint probability density function (or, for discrete data, probability mass function), $p(t, \\mathbf{x})$.\n",
    "\n",
    "We shall use the machine learning model \n",
    "\n",
    "$$f(\\mathbf{x}, \\theta) = \\mbox{sigmoid}(\\,\\mathbf{b}_1 + \\mathbf{w}_1 \\, \\mbox{relu}(\\mathbf{b}_0 + \\mathbf{w}_0 \\, \\mathbf{x}) \\, ),$$\n",
    "\n",
    "where $\\mathbf{b}$ and $\\mathbf{w}$ (the biases and weights) are the parameters $\\theta$ of the model and $\\mbox{relu}(x)$ is a function applied to every element $x_i$ of its tensor argument (i.e., applied element-wise) defined by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{relu}(x) & = \\begin{cases}\n",
    "    x, & \\text{if } x \\gt 0\\\\\n",
    "    0              & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The functions sigmoid and relu are implemented in the PyTorch functions\n",
    "torch.nn.__Sigmoid__() and torch.nn.__ReLU__(), respectively. In this exercise, $\\mathbf{b}_1$ is a scalar since we have just one output.\n",
    "\n",
    "\n",
    "## Recap: Bayes' Theorem\n",
    "\n",
    "Let\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    p(A) & = \\textrm{Probability of } A,\\\\\n",
    "    p(B) & = \\textrm{Probability of } B,\\\\\n",
    "    \\textrm{and } p(AB) = p(A \\cap B ) & = \\textrm{Probability of } A \\textrm{ and } B,\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "then,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    p(A | B) & = \\frac{p(AB)}{p(B)},\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "is called the *conditional probability* of $A$ given $B$. This is answering the question: what is the chance that $A$ is true given that $B$ is true. For example, let $B = \\textrm{it is a cat}$ and let $A = \\textrm{the fur is white}$, then $p(A | B)$ is the probability that the fur is white given that it is a cat.\n",
    "\n",
    "From the definition of conditional probability, it follows that\n",
    "$p(AB) = p(A | B) \\, p(B)$. But, by swapping $A$ and $B$ on both sides of that equation, it is also true that $p(BA) = p(B | A) \\, p(A)$. But, since $p(AB) = p(BA)$, this leads to \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    p(\\color{red}{B} | A ) & = \\frac{p(A | \\color{red}{B}) \\, p(\\color{red}{B})}{p(A)},\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "which is __Bayes' Theorem__ (Thomas Bayes, 1763).\n",
    "\n",
    "Suppose we have multiple $B$s, $B_k$, $k = 1, \\cdots$ and the set of $B$s is *exhaustive*, that is, it contains all possible (relevant) $B_k$, then Bayes' theorem becomes\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    p(B_k | A ) & = \\frac{p(A | B_k) \\, p(B_k)}{p(A)}.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Because the set of $B_k$ is *exhaustive*, it follows that \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    \\sum_k p(B_k | A ) & = 1.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Consequently, $p(A)$ must satisfy the sum rule\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "        p(A) & = \\sum_k p(A | B_k) \\, p(B_k), \\\\\n",
    "            & = \\sum_k p(A B_k) .\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "## Loss function\n",
    "Typically, a machine learning model is fitted to the training data by minimizing a suitably defined function, which in the statistics literature is often referred to as the __empirical risk__. (Fitting models to data is called learning by machine learning enthusiasts.) The empirical risk is a Monte Carlo approximation of the __risk functional__, defined by \n",
    "\n",
    "\\begin{align*}\n",
    "    R[f] &= \\int \\cdots \\int L(t, \\, f(\\mathbf{x}, \\theta)) \\, \n",
    "    p(t, \\, \\mathbf{x}) \\, dt \\, d\\mathbf{x},\n",
    "\\end{align*}\n",
    "\n",
    "where $L(t, \\, f)$ is called the __loss function__ and measures how much one loses if the output of the parameterized function $f(x, \\theta)$ differs from the __target__ $t$. $\\color{blue}{\\rm Warning}$: In the machine learning world, the empirical risk is typicaly referred to as the loss function, when what is really meant is the average loss function. \n",
    "\n",
    "__Important Note__: In order for the risk functional $R[f]$ to reach its minimum, defined by variations of $f$ that yield the condition $\\delta R = 0$ $\\forall \\, x$, the function $f$ must be sufficiently flexible. If the latter condition is satisfied, then the mathematical quantity approximated by $f$ depends solely on the form of the loss function $L(t, \\, f)$ and the probability distribution $p(t, \\, x)$ of the training data. In particular, it does not depend on\n",
    "    the details of the functon $f$ apart from its presumed flexibility. Of course, in practice, we do not minimize $R[f]$, but rather the empirical risk, which approximates it. Nevertheless, to the degree that a very large data set approximates an infinite one and to the degree that our minimizer is able to find a good approximation to the minimum, this bit of reasoning suggests that it is as least as important to think about the form of the loss function $L(t, \\, f)$ as it is to think about the form of the model. If we have two models of equal functional flexibility then, *a priori*, for the same loss function the models will approximate the same quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FA1Y5VCv20XZ"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  a function to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "#  split data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.528435</td>\n",
       "      <td>2.482336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.298034</td>\n",
       "      <td>0.350602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844025</td>\n",
       "      <td>0.138175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.280748</td>\n",
       "      <td>1.273752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519784</td>\n",
       "      <td>-1.486675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553385</td>\n",
       "      <td>1.235875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015297</td>\n",
       "      <td>-1.244069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933215</td>\n",
       "      <td>0.806885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.748572</td>\n",
       "      <td>0.978152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062602</td>\n",
       "      <td>-1.167001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         x         y\n",
       "0     0.0 -1.528435  2.482336\n",
       "1     1.0 -0.298034  0.350602\n",
       "2     1.0  0.844025  0.138175\n",
       "3     0.0 -1.280748  1.273752\n",
       "4     0.0  0.519784 -1.486675\n",
       "5     1.0  0.553385  1.235875\n",
       "6     0.0 -0.015297 -1.244069\n",
       "7     1.0  0.933215  0.806885\n",
       "8     1.0  1.748572  0.978152\n",
       "9     0.0  1.062602 -1.167001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = jb.load('data.db')\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test sets\n",
    "There is some confusion in terminology regarding validation and test samples (or sets). We shall adhere to the defintions given here https://machinelearningmastery.com/difference-test-validation-datasets/):\n",
    "   \n",
    "  * __Training Dataset__: The sample of data used to fit the model.\n",
    "  * __Validation Dataset__: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "  * __Test Dataset__: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "The validation set will be some small fraction of the training set and will be used to decide when to stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:         35000\n",
      "validation set size:     5000\n",
      "test set size:          10000\n"
     ]
    }
   ],
   "source": [
    "fraction = 0.20\n",
    "train_data, test_data = train_test_split(data, test_size=fraction)\n",
    "\n",
    "fraction = 0.125\n",
    "train_data, valid_data = train_test_split(train_data, test_size=fraction)\n",
    "\n",
    "print('train set size:        %6d' % train_data.shape[0])\n",
    "print('validation set size:   %6d' % valid_data.shape[0])\n",
    "print('test set size:         %6d' % test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into targets $t$ and inputs $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.8759602   1.6548501 ]\n",
      " [-0.5391906  -0.89200723]\n",
      " [ 0.43230608 -1.3800026 ]]\n",
      "\n",
      "[[ 0.7940733   1.4197329 ]\n",
      " [ 0.26457557 -1.9704969 ]\n",
      " [-0.20728141 -1.3524994 ]]\n",
      "\n",
      "[[-0.46819124 -1.0124339 ]\n",
      " [ 0.17062046  1.2169482 ]\n",
      " [ 2.65047    -1.8208842 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_t_x(df, inputs=['x', 'y']):\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    t = df['target'].to_numpy().astype(np.long)\n",
    "    x = df[inputs].to_numpy().astype(np.float32)\n",
    "    return (t, x)\n",
    "\n",
    "train_t, train_x = split_t_x(train_data)\n",
    "valid_t, valid_x = split_t_x(valid_data)\n",
    "test_t,  test_x  = split_t_x(test_data)\n",
    "\n",
    "print(train_x[:3], end='\\n\\n') \n",
    "print(valid_x[:3], end='\\n\\n')\n",
    "print(test_x[:3],  end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([1, 0])\n",
      "tensor([[2.],\n",
      "        [3.]])\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# create a float tensor\n",
    "a = torch.Tensor([[1,2], [3,4]])\n",
    "print(a)\n",
    "\n",
    "# create an integer tensor, which we'll use for gathering  \n",
    "# elements of the [2,2] tensor 'a' along its dim=1 \n",
    "# (that is, horizontal) dimension.\n",
    "i = torch.tensor([1,0])\n",
    "print(i)\n",
    "\n",
    "# note that since we're gathering along the dim=1 dimension\n",
    "# of 'a' the tensor 'i' must be of the same shape as 'a'\n",
    "# along other dimensions besides dim=1, that is, along dim=0.\n",
    "# by using view(-1,...) we make the dim=0 (vertical)\n",
    "# dimension of i be of the same length as the dim=0 \n",
    "# dimension of 'a'.\n",
    "b  = a.gather(dim=1, index=i.view(-1, 1))\n",
    "print(b)\n",
    "\n",
    "# now we get rid of extraneous dimensions.\n",
    "# compare tensors 'b' and 'c'. in 'b' there is only one \n",
    "# element per row along dim=1, so its shape is [2, 1]. \n",
    "# We are, therefore, free to squeeze away the dim=1 to\n",
    "# arrive at a new tensor 'c' with shape [2,]\n",
    "c = b.squeeze()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return a (random) batch of data from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"number\" integers from the range [0, length-1]\n",
    "    rows    = rnd.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    return (batch_x, batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical risk (that is, average loss)\n",
    "\n",
    "The empirical risk, which is the __objective function__ we shall minimize, is defined as\n",
    "\n",
    "\\begin{align}\n",
    "R_M(\\theta) & = \\frac{1}{M}\\sum_{m=1}^M L(t_m, f_m),\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "    f_m & \\equiv f(\\mathbf{x}_m, \\theta),\\\\ \\\\ \\textrm{and} \\\\\n",
    "    L(t, f) &= -\\ln[f^t \\, (1 - f)^{1 - t}], \\\\\n",
    "            &= \n",
    "\\begin{cases}\n",
    "    -\\ln f, & \\text{if } t == 1\\\\\n",
    "    -\\ln (1-f)             & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "The empirical risk $R_M$ approximates the __risk__\n",
    "\n",
    "\\begin{align}\n",
    "R[f] & = \\int \\cdots \\int \\, p(t, \\mathbf{x}) \\, L(t, f(\\mathbf{x}, \\theta)) \\, dt \\, d\\mathbf{x},\n",
    "\\end{align}\n",
    "\n",
    "which is a __functional__ of the model $f$. The quantity $p(t, \\mathbf{x}) \\, dt\\, d\\mathbf{x}$ is the probability distribution from which the sample $\\{ (t_m, \\mathbf{x}_m), m = 1,\\cdots, M \\}$ is presumed to have been drawn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_loss(f, t):\n",
    "    losses =-torch.where(t > 0.5, torch.log(f), torch.log(1-f))   \n",
    "    return torch.mean(losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define accuracy of model\n",
    "\n",
    "Let's define accuracy as the fraction of the sample that is correctly classified. Recall, that if we use either the __quadratic loss__ \n",
    "\n",
    "$$L(t, f) = (t - f)^2$$\n",
    "\n",
    "or the __cross entropy loss__\n",
    "\n",
    "$$L(t, f) =- [t \\, \\ln(f) + (1 - t) \\ln(1-f)],$$\n",
    "\n",
    "the model $f(\\mathbf{x}, \\theta)$ will approximate the mean of $p(t | \\mathbf{x})$, that is,\n",
    "\n",
    "$$f(\\mathbf{x}, \\theta^*) \\approx p(1 | \\mathbf{x}) \n",
    "= \\frac{p(\\mathbf{x} | 1) \\, p(1)}{p(\\mathbf{x} | 1) \\, p(1) + p(\\mathbf{x} | 0) \\, p(0)},$$\n",
    "\n",
    "provided that\n",
    "   1. We have a lot of training data (strictly, an infinite amount).\n",
    "   2. We have a sufficienty flexible model $\\forall \\, \\mathbf{x}$.\n",
    "   3. We have an effective optimizer so that we can find the best fit parameter point $\\theta^*$.\n",
    "   \n",
    "  \n",
    "If an object, characterized by $\\mathbf{x}$, yields $f(\\mathbf{x}, \\theta^*) \\approx p(1 | \\mathbf{x}) > 0.5$ we'll assign it to the class with $t = 1$, otherwise it is assigned to the class with $t = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(f, t):\n",
    "    # Count how many times the predicted target matches \n",
    "    # the true target and convert to a fraction.\n",
    "    s = (f > 0.5).long()   # convert to long ints\n",
    "    a = (s == t).float()   # convert to floats\n",
    "    return float(torch.mean(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to execute training loop\n",
    "\n",
    "Note, here we use $t$ and $y$ interchangeably to denote the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, avloss, getbatch,\n",
    "          train_x, train_y, \n",
    "          valid_x, valid_y,\n",
    "          batch_size, \n",
    "          n_iterations, step=10):\n",
    "    \n",
    "    xx   = []\n",
    "    yy_t = []\n",
    "    yy_v = []\n",
    "    n    = 5000\n",
    "    \n",
    "    # set mode to training so that training specific operations such \n",
    "    # as dropout are enabled.\n",
    "    model.train()\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "\n",
    "        # Get a random sample (a batch) of images (as numpy arrays)\n",
    "        batch_x, batch_y = getbatch(train_x, train_y, batch_size)\n",
    "        \n",
    "        # Convert the numpy arrays batch_x and batch_y, to tensor \n",
    "        # types. The PyTorch tensor type is the magic that permits \n",
    "        # automatic differentiation with respect to parameters. \n",
    "        # However, since we do not need to take the derivatives\n",
    "        # with respect to x and y, we disable this feature\n",
    "        with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "            x = torch.from_numpy(batch_x)\n",
    "            y = torch.from_numpy(batch_y)      \n",
    "\n",
    "        # compute the output of the model for the batch of data x\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk = avloss(outputs, y)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        empirical_risk.backward()   # compute gradients\n",
    "        \n",
    "        # Finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer.step()            # move one step\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            acc_t = validate(model, train_x[:n], train_y[:n]) \n",
    "            acc_v = validate(model, valid_x[:n], valid_y[:n])\n",
    "            \n",
    "            print(\"\\r%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v), end='')\n",
    "        \n",
    "            xx.append(ii)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "    return (xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, xx, yy):\n",
    "    # make sure we set evaluation mode so that training specific\n",
    "    # operations such as dropout are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "        # compute accuracy using training sample\n",
    "        x = torch.from_numpy(xx)\n",
    "        y = torch.from_numpy(yy)\n",
    "        o = model(x)\n",
    "        acc = accuracy(o, y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(traces):\n",
    "    \n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    # adjust y limits\n",
    "    axes = ax.axes\n",
    "    axes.set_ylim((0, 1))\n",
    "    axes.set_xlim((0, xx[-1]))\n",
    "    \n",
    "    plt.plot(xx, yy_t, 'b', label='Training')\n",
    "    plt.plot(xx, yy_v, 'r', label='Validation')\n",
    "    plt.title('Training and Validation Errors')\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.grid(True, which=\"both\", linestyle='-')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJC916BU-9L6"
   },
   "source": [
    "### Define model $f(\\mathbf{x}, \\theta)$\n",
    "\n",
    "The output non-linearity is chosen to be a __sigmoid__ function:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + \\exp(-z)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRVXyg9A1zQh",
    "outputId": "aa498166-4008-461a-c82a-9eab5b49b083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "N_INPUTS  = 2       # number of inputs (x1, x2)\n",
    "N_NODES_0 = 2       # number of nodes in layer 0\n",
    "N_NODES_1 = 1       # number of nodes in layer 1 (output layer)     \n",
    "\n",
    "# Instead of create our own class, let's just use the Sequential class.\n",
    "# Try to guess what's going on here.\n",
    "model = torch.nn.Sequential(\n",
    "             torch.nn.Linear(N_INPUTS,  N_NODES_0),\n",
    "             torch.nn.ReLU(),\n",
    "             torch.nn.Linear(N_NODES_0, N_NODES_1),\n",
    "             torch.nn.Sigmoid()\n",
    "     )\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8R55NgQL4EC"
   },
   "source": [
    "Instantiate an optimizer, then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       199\t    0.5389\t    0.5383"
     ]
    }
   ],
   "source": [
    "n_batch = 10\n",
    "n_iterations = 200\n",
    "learning_rate = 0.003\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "traces = train(model, optimizer, \n",
    "               average_loss,\n",
    "               get_batch,\n",
    "               train_x, train_t, \n",
    "               valid_x, valid_t,\n",
    "               n_batch, \n",
    "               n_iterations,\n",
    "               step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_19_REINFORCE_Algorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
